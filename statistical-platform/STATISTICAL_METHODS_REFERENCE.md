# ğŸ“Š í†µê³„ ë¶„ì„ ë°©ë²• ì°¸ì¡° ë¬¸ì„œ
## Statistical Methods Reference

**ë²„ì „**: 2.0  
**ì—…ë°ì´íŠ¸**: 2025-09-12  
**ëª©ì **: Pyodide + SciPy/statsmodels ê¸°ë°˜ í†µê³„ ë¶„ì„ ì™„ì „ ê°€ì´ë“œ

---

## ğŸ¯ í†µê³„ ì²˜ë¦¬ ì•„í‚¤í…ì²˜

### í•µì‹¬ ì›ì¹™
```
ëª¨ë“  í†µê³„ ê³„ì‚° = Pyodide (Python WebAssembly) + SciPy/statsmodels
âŒ JavaScript/TypeScriptë¡œ í†µê³„ í•¨ìˆ˜ ì§ì ‘ êµ¬í˜„ ê¸ˆì§€
âœ… ê²€ì¦ëœ ê³¼í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© í•„ìˆ˜
```

### ê¸°ìˆ  ìŠ¤íƒ
- **Python ëŸ°íƒ€ì„**: Pyodide 0.28.2 (WebAssembly)
- **ê¸°ë³¸ í†µê³„**: SciPy 1.14.1 (scipy.stats)
- **ê³ ê¸‰ í†µê³„**: statsmodels 0.14.6
- **ìˆ˜ì¹˜ ê³„ì‚°**: NumPy 2.2.1
- **ë°ì´í„° ì²˜ë¦¬**: pandas 2.2.3

---

## ğŸ“ˆ ê¸°ìˆ í†µê³„ (Descriptive Statistics)

### SciPy ê¸°ë°˜ êµ¬í˜„
```python
import scipy.stats as stats
import numpy as np

def calculate_descriptive_stats(data):
    """ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚° - SciPy ì‚¬ìš©"""
    data = np.array(data)
    
    return {
        'count': len(data),
        'mean': np.mean(data),
        'median': np.median(data),
        'mode': stats.mode(data, keepdims=True).mode[0] if len(stats.mode(data).mode) > 0 else None,
        'std': np.std(data, ddof=1),  # í‘œë³¸í‘œì¤€í¸ì°¨
        'var': np.var(data, ddof=1),  # í‘œë³¸ë¶„ì‚°
        'min': np.min(data),
        'max': np.max(data),
        'range': np.ptp(data),  # peak to peak
        'q1': np.percentile(data, 25),
        'q3': np.percentile(data, 75),
        'iqr': stats.iqr(data),
        'skewness': stats.skew(data),
        'kurtosis': stats.kurtosis(data),
        'cv': stats.variation(data)  # ë³€ë™ê³„ìˆ˜
    }
```

### JavaScriptì—ì„œ í˜¸ì¶œ
```javascript
const result = await pyodide.runPython(`
    import json
    result = calculate_descriptive_stats([1, 2, 3, 4, 5])
    json.dumps(result)
`)
```

---

## ğŸ“Š ì¶”ë¡ í†µê³„ (Inferential Statistics)

### 1. T-ê²€ì • (T-Tests)

#### ì¼í‘œë³¸ t-ê²€ì •
```python
def one_sample_ttest(data, population_mean=0, alpha=0.05):
    """ì¼í‘œë³¸ t-ê²€ì •"""
    statistic, pvalue = stats.ttest_1samp(data, population_mean)
    
    n = len(data)
    df = n - 1
    sample_mean = np.mean(data)
    sample_std = np.std(data, ddof=1)
    se = sample_std / np.sqrt(n)
    
    # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
    t_critical = stats.t.ppf(1 - alpha/2, df)
    margin_of_error = t_critical * se
    ci = [sample_mean - margin_of_error, sample_mean + margin_of_error]
    
    # Cohen's d (íš¨ê³¼í¬ê¸°)
    cohens_d = (sample_mean - population_mean) / sample_std
    
    return {
        'test_name': 'One-Sample t-test',
        'statistic': float(statistic),
        'p_value': float(pvalue),
        'degrees_of_freedom': df,
        'sample_mean': float(sample_mean),
        'population_mean': population_mean,
        'confidence_interval': [float(ci[0]), float(ci[1])],
        'effect_size_cohens_d': float(cohens_d),
        'is_significant': pvalue < alpha
    }
```

#### ë…ë¦½í‘œë³¸ t-ê²€ì •
```python
def independent_ttest(group1, group2, equal_var=True, alpha=0.05):
    """ë…ë¦½í‘œë³¸ t-ê²€ì •"""
    if equal_var:
        statistic, pvalue = stats.ttest_ind(group1, group2)
        test_name = 'Independent t-test (equal variances)'
    else:
        statistic, pvalue = stats.ttest_ind(group1, group2, equal_var=False)
        test_name = "Welch's t-test (unequal variances)"
    
    n1, n2 = len(group1), len(group2)
    df = n1 + n2 - 2
    
    # íš¨ê³¼í¬ê¸° (Cohen's d)
    pooled_std = np.sqrt(((n1-1)*np.var(group1, ddof=1) + (n2-1)*np.var(group2, ddof=1)) / df)
    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std
    
    return {
        'test_name': test_name,
        'statistic': float(statistic),
        'p_value': float(pvalue),
        'degrees_of_freedom': df,
        'group1_mean': float(np.mean(group1)),
        'group2_mean': float(np.mean(group2)),
        'mean_difference': float(np.mean(group1) - np.mean(group2)),
        'effect_size_cohens_d': float(cohens_d),
        'is_significant': pvalue < alpha
    }
```

#### ëŒ€ì‘í‘œë³¸ t-ê²€ì •
```python
def paired_ttest(before, after, alpha=0.05):
    """ëŒ€ì‘í‘œë³¸ t-ê²€ì •"""
    statistic, pvalue = stats.ttest_rel(before, after)
    
    differences = np.array(after) - np.array(before)
    n = len(differences)
    df = n - 1
    mean_diff = np.mean(differences)
    
    # íš¨ê³¼í¬ê¸°
    cohens_d = mean_diff / np.std(differences, ddof=1)
    
    return {
        'test_name': 'Paired t-test',
        'statistic': float(statistic),
        'p_value': float(pvalue),
        'degrees_of_freedom': df,
        'mean_difference': float(mean_diff),
        'effect_size_cohens_d': float(cohens_d),
        'is_significant': pvalue < alpha
    }
```

### 2. ë¶„ì‚°ë¶„ì„ (ANOVA)

#### ì¼ì›ë¶„ì‚°ë¶„ì„
```python
def one_way_anova(*groups, alpha=0.05):
    """ì¼ì›ë¶„ì‚°ë¶„ì„"""
    # F-ê²€ì •
    f_statistic, p_value = stats.f_oneway(*groups)
    
    # ììœ ë„
    k = len(groups)  # ê·¸ë£¹ ìˆ˜
    n_total = sum(len(group) for group in groups)
    df_between = k - 1
    df_within = n_total - k
    
    # ì œê³±í•© ê³„ì‚°
    grand_mean = np.mean(np.concatenate(groups))
    
    ss_between = sum(len(group) * (np.mean(group) - grand_mean)**2 for group in groups)
    ss_within = sum(np.sum((group - np.mean(group))**2) for group in groups)
    ss_total = ss_between + ss_within
    
    # í‰ê· ì œê³±
    ms_between = ss_between / df_between
    ms_within = ss_within / df_within
    
    # íš¨ê³¼í¬ê¸° (eta-squared)
    eta_squared = ss_between / ss_total
    
    return {
        'test_name': 'One-Way ANOVA',
        'f_statistic': float(f_statistic),
        'p_value': float(p_value),
        'df_between': df_between,
        'df_within': df_within,
        'ss_between': float(ss_between),
        'ss_within': float(ss_within),
        'ss_total': float(ss_total),
        'ms_between': float(ms_between),
        'ms_within': float(ms_within),
        'eta_squared': float(eta_squared),
        'is_significant': p_value < alpha
    }
```

#### ì´ì›ë¶„ì‚°ë¶„ì„ (statsmodels ì‚¬ìš©)
```python
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

def two_way_anova(data, dependent_var, factor1, factor2, interaction=True):
    """ì´ì›ë¶„ì‚°ë¶„ì„ - statsmodels ì‚¬ìš©"""
    import pandas as pd
    
    df = pd.DataFrame(data)
    
    if interaction:
        formula = f"{dependent_var} ~ C({factor1}) + C({factor2}) + C({factor1}):C({factor2})"
    else:
        formula = f"{dependent_var} ~ C({factor1}) + C({factor2})"
    
    model = ols(formula, data=df).fit()
    anova_results = anova_lm(model, typ=2)
    
    return {
        'test_name': 'Two-Way ANOVA',
        'anova_table': anova_results.to_dict(),
        'model_summary': str(model.summary())
    }
```

### 3. ì‚¬í›„ê²€ì • (Post-hoc Tests)

#### Tukey HSD
```python
from statsmodels.stats.multicomp import pairwise_tukeyhsd

def tukey_hsd_test(data, groups, alpha=0.05):
    """Tukey HSD ì‚¬í›„ê²€ì •"""
    tukey_result = pairwise_tukeyhsd(data, groups, alpha=alpha)
    
    return {
        'test_name': 'Tukey HSD',
        'summary': str(tukey_result.summary()),
        'pairwise_comparisons': tukey_result.summary().data[1:]  # í—¤ë” ì œì™¸
    }
```

#### Games-Howell (ë¶ˆê· ë“± ë¶„ì‚°)
```python
def games_howell_test(*groups, alpha=0.05):
    """Games-Howell ì‚¬í›„ê²€ì • (ë¶ˆê· ë“±ë¶„ì‚°)"""
    from itertools import combinations
    
    results = []
    group_names = [f'Group_{i+1}' for i in range(len(groups))]
    
    for i, (g1, g2) in enumerate(combinations(range(len(groups)), 2)):
        group1, group2 = groups[g1], groups[g2]
        
        # Welch's t-test
        stat, pval = stats.ttest_ind(group1, group2, equal_var=False)
        
        # ììœ ë„ (Welch-Satterthwaite)
        n1, n2 = len(group1), len(group2)
        s1, s2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
        
        df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))
        
        results.append({
            'comparison': f'{group_names[g1]} vs {group_names[g2]}',
            'mean_diff': float(np.mean(group1) - np.mean(group2)),
            'statistic': float(stat),
            'p_value': float(pval),
            'degrees_of_freedom': float(df)
        })
    
    return {
        'test_name': 'Games-Howell',
        'comparisons': results
    }
```

---

## ğŸ“ˆ ìƒê´€ë¶„ì„ (Correlation Analysis)

### Pearson ìƒê´€ê³„ìˆ˜
```python
def pearson_correlation(x, y, alpha=0.05):
    """Pearson ìƒê´€ë¶„ì„"""
    correlation, p_value = stats.pearsonr(x, y)
    
    n = len(x)
    df = n - 2
    
    # ì‹ ë¢°êµ¬ê°„ (Fisher's r-to-z transformation)
    z = np.arctanh(correlation)
    se_z = 1 / np.sqrt(n - 3)
    z_critical = stats.norm.ppf(1 - alpha/2)
    
    z_lower = z - z_critical * se_z
    z_upper = z + z_critical * se_z
    
    ci_lower = np.tanh(z_lower)
    ci_upper = np.tanh(z_upper)
    
    return {
        'correlation_type': 'Pearson',
        'correlation': float(correlation),
        'p_value': float(p_value),
        'confidence_interval': [float(ci_lower), float(ci_upper)],
        'sample_size': n,
        'degrees_of_freedom': df,
        'is_significant': p_value < alpha
    }
```

### Spearman ìˆœìœ„ìƒê´€
```python
def spearman_correlation(x, y, alpha=0.05):
    """Spearman ìˆœìœ„ìƒê´€ë¶„ì„"""
    correlation, p_value = stats.spearmanr(x, y)
    
    return {
        'correlation_type': 'Spearman',
        'correlation': float(correlation),
        'p_value': float(p_value),
        'sample_size': len(x),
        'is_significant': p_value < alpha
    }
```

---

## ğŸ“Š íšŒê·€ë¶„ì„ (Regression Analysis)

### ë‹¨ìˆœì„ í˜•íšŒê·€
```python
def simple_linear_regression(x, y, alpha=0.05):
    """ë‹¨ìˆœì„ í˜•íšŒê·€ - SciPy ì‚¬ìš©"""
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    
    n = len(x)
    df = n - 2
    
    # ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨
    y_pred = slope * np.array(x) + intercept
    residuals = np.array(y) - y_pred
    
    # R-squared
    r_squared = r_value ** 2
    
    # í‘œì¤€ì˜¤ì°¨ë“¤
    mse = np.sum(residuals**2) / df
    se_slope = std_err
    se_intercept = np.sqrt(mse * (1/n + np.mean(x)**2 / np.sum((x - np.mean(x))**2)))
    
    return {
        'regression_type': 'Simple Linear',
        'slope': float(slope),
        'intercept': float(intercept),
        'r_value': float(r_value),
        'r_squared': float(r_squared),
        'p_value': float(p_value),
        'std_err_slope': float(se_slope),
        'std_err_intercept': float(se_intercept),
        'residuals': residuals.tolist(),
        'fitted_values': y_pred.tolist(),
        'is_significant': p_value < alpha
    }
```

### ë‹¤ì¤‘ì„ í˜•íšŒê·€ (statsmodels)
```python
import statsmodels.api as sm

def multiple_linear_regression(X, y):
    """ë‹¤ì¤‘ì„ í˜•íšŒê·€ - statsmodels ì‚¬ìš©"""
    # ìƒìˆ˜í•­ ì¶”ê°€
    X_with_const = sm.add_constant(X)
    
    # ëª¨í˜• ì í•©
    model = sm.OLS(y, X_with_const).fit()
    
    return {
        'regression_type': 'Multiple Linear',
        'coefficients': model.params.to_dict(),
        'r_squared': float(model.rsquared),
        'adjusted_r_squared': float(model.rsquared_adj),
        'f_statistic': float(model.fvalue),
        'f_pvalue': float(model.f_pvalue),
        'p_values': model.pvalues.to_dict(),
        'confidence_intervals': model.conf_int().to_dict(),
        'residuals': model.resid.tolist(),
        'fitted_values': model.fittedvalues.tolist(),
        'summary': str(model.summary())
    }
```

---

## ğŸ” ë¹„ëª¨ìˆ˜ ê²€ì • (Non-parametric Tests)

### Mann-Whitney U ê²€ì •
```python
def mann_whitney_u_test(group1, group2, alpha=0.05):
    """Mann-Whitney U ê²€ì • (ë…ë¦½í‘œë³¸)"""
    statistic, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')
    
    n1, n2 = len(group1), len(group2)
    
    # íš¨ê³¼í¬ê¸° (r = Z / sqrt(N))
    z_score = (statistic - n1*n2/2) / np.sqrt(n1*n2*(n1+n2+1)/12)
    effect_size_r = abs(z_score) / np.sqrt(n1 + n2)
    
    return {
        'test_name': 'Mann-Whitney U',
        'u_statistic': float(statistic),
        'p_value': float(p_value),
        'z_score': float(z_score),
        'effect_size_r': float(effect_size_r),
        'sample_sizes': [n1, n2],
        'is_significant': p_value < alpha
    }
```

### Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •
```python
def wilcoxon_signed_rank_test(before, after, alpha=0.05):
    """Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì • (ëŒ€ì‘í‘œë³¸)"""
    statistic, p_value = stats.wilcoxon(before, after)
    
    differences = np.array(after) - np.array(before)
    n = len(differences[differences != 0])  # 0ì´ ì•„ë‹Œ ì°¨ì´ì˜ ê°œìˆ˜
    
    # íš¨ê³¼í¬ê¸°
    z_score = (statistic - n*(n+1)/4) / np.sqrt(n*(n+1)*(2*n+1)/24)
    effect_size_r = abs(z_score) / np.sqrt(n)
    
    return {
        'test_name': 'Wilcoxon Signed-Rank',
        'w_statistic': float(statistic),
        'p_value': float(p_value),
        'z_score': float(z_score),
        'effect_size_r': float(effect_size_r),
        'sample_size': n,
        'is_significant': p_value < alpha
    }
```

### Kruskal-Wallis ê²€ì •
```python
def kruskal_wallis_test(*groups, alpha=0.05):
    """Kruskal-Wallis ê²€ì • (ë¹„ëª¨ìˆ˜ ì¼ì›ë¶„ì‚°ë¶„ì„)"""
    statistic, p_value = stats.kruskal(*groups)
    
    n_total = sum(len(group) for group in groups)
    k = len(groups)
    df = k - 1
    
    # íš¨ê³¼í¬ê¸° (eta-squared)
    eta_squared = (statistic - k + 1) / (n_total - k)
    
    return {
        'test_name': 'Kruskal-Wallis',
        'h_statistic': float(statistic),
        'p_value': float(p_value),
        'degrees_of_freedom': df,
        'eta_squared': float(eta_squared),
        'group_count': k,
        'total_sample_size': n_total,
        'is_significant': p_value < alpha
    }
```

---

## âœ… ê°€ì • ê²€ì • (Assumption Tests)

### ì •ê·œì„± ê²€ì •
```python
def normality_tests(data, alpha=0.05):
    """ì •ê·œì„± ê²€ì • ë°°í„°ë¦¬"""
    n = len(data)
    
    results = {}
    
    # Shapiro-Wilk (n â‰¤ 5000)
    if n <= 5000:
        sw_stat, sw_p = stats.shapiro(data)
        results['shapiro_wilk'] = {
            'statistic': float(sw_stat),
            'p_value': float(sw_p),
            'is_normal': sw_p > alpha
        }
    
    # Kolmogorov-Smirnov
    ks_stat, ks_p = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
    results['kolmogorov_smirnov'] = {
        'statistic': float(ks_stat),
        'p_value': float(ks_p),
        'is_normal': ks_p > alpha
    }
    
    # D'Agostino-Pearson
    dp_stat, dp_p = stats.normaltest(data)
    results['dagostino_pearson'] = {
        'statistic': float(dp_stat),
        'p_value': float(dp_p),
        'is_normal': dp_p > alpha
    }
    
    return results
```

### ë“±ë¶„ì‚°ì„± ê²€ì •
```python
def homogeneity_tests(*groups, alpha=0.05):
    """ë“±ë¶„ì‚°ì„± ê²€ì •"""
    results = {}
    
    # Levene's test (robust)
    levene_stat, levene_p = stats.levene(*groups)
    results['levene'] = {
        'statistic': float(levene_stat),
        'p_value': float(levene_p),
        'homogeneous': levene_p > alpha
    }
    
    # Bartlett's test (assumes normality)
    bartlett_stat, bartlett_p = stats.bartlett(*groups)
    results['bartlett'] = {
        'statistic': float(bartlett_stat),
        'p_value': float(bartlett_p),
        'homogeneous': bartlett_p > alpha
    }
    
    return results
```

---

## ğŸ¯ ìˆ˜ì‚°ê³¼í•™ íŠ¹í™” ë¶„ì„

### CPUE ë¶„ì„
```python
def cpue_analysis(catch_data, effort_data):
    """ë‹¨ìœ„ë…¸ë ¥ë‹¹ì–´íšëŸ‰(CPUE) ë¶„ì„"""
    cpue = np.array(catch_data) / np.array(effort_data)
    
    # ê¸°ë³¸ í†µê³„
    basic_stats = calculate_descriptive_stats(cpue)
    
    # íŠ¸ë Œë“œ ë¶„ì„ (ì„ í˜•íšŒê·€)
    time_points = np.arange(len(cpue))
    slope, intercept, r_value, p_value, std_err = stats.linregress(time_points, cpue)
    
    return {
        'cpue_values': cpue.tolist(),
        'basic_statistics': basic_stats,
        'trend_analysis': {
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(r_value**2),
            'p_value': float(p_value),
            'trend_direction': 'increasing' if slope > 0 else 'decreasing',
            'is_significant_trend': p_value < 0.05
        }
    }
```

### von Bertalanffy ì„±ì¥ëª¨ë¸
```python
from scipy.optimize import curve_fit

def von_bertalanffy_growth(ages, lengths):
    """von Bertalanffy ì„±ì¥ëª¨ë¸ ì í•©"""
    
    def vb_function(t, Linf, K, t0):
        """von Bertalanffy í•¨ìˆ˜"""
        return Linf * (1 - np.exp(-K * (t - t0)))
    
    # ì´ˆê¸°ê°’ ì¶”ì •
    Linf_initial = max(lengths) * 1.2
    K_initial = 0.1
    t0_initial = 0
    
    # ëª¨ë¸ ì í•©
    popt, pcov = curve_fit(vb_function, ages, lengths, 
                          p0=[Linf_initial, K_initial, t0_initial])
    
    Linf, K, t0 = popt
    
    # ì‹ ë¢°êµ¬ê°„
    param_std = np.sqrt(np.diag(pcov))
    
    # ì í•©ë„
    predicted = vb_function(np.array(ages), *popt)
    r_squared = 1 - np.sum((lengths - predicted)**2) / np.sum((lengths - np.mean(lengths))**2)
    
    return {
        'parameters': {
            'Linf': float(Linf),
            'K': float(K),
            't0': float(t0)
        },
        'parameter_std_errors': {
            'Linf_se': float(param_std[0]),
            'K_se': float(param_std[1]),
            't0_se': float(param_std[2])
        },
        'fitted_values': predicted.tolist(),
        'r_squared': float(r_squared),
        'equation': f'Lt = {Linf:.2f} * (1 - exp(-{K:.3f} * (t - {t0:.2f})))'
    }
```

---

## ğŸ”§ ê²€ì •ë ¥ ë¶„ì„ (Power Analysis)

### t-ê²€ì • ê²€ì •ë ¥
```python
from statsmodels.stats.power import ttest_power, tt_solve_power

def t_test_power_analysis(effect_size, sample_size=None, power=None, alpha=0.05):
    """t-ê²€ì • ê²€ì •ë ¥ ë¶„ì„"""
    
    if power is None:
        # ê²€ì •ë ¥ ê³„ì‚°
        power = ttest_power(effect_size, sample_size, alpha)
        return {
            'analysis_type': 'Power Calculation',
            'effect_size': float(effect_size),
            'sample_size': sample_size,
            'alpha': alpha,
            'power': float(power)
        }
    elif sample_size is None:
        # í‘œë³¸ í¬ê¸° ê³„ì‚°
        sample_size = tt_solve_power(effect_size=effect_size, power=power, alpha=alpha)
        return {
            'analysis_type': 'Sample Size Calculation',
            'effect_size': float(effect_size),
            'required_sample_size': int(np.ceil(sample_size)),
            'alpha': alpha,
            'power': power
        }
```

---

## ğŸ“Š JavaScript í†µí•© ì˜ˆì œ

### ì™„ì „í•œ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
```javascript
// Pyodide ë¡œë“œ ë° ë¶„ì„ ì‹¤í–‰
async function runCompleteAnalysis(data, testType) {
    // Python ë¶„ì„ ì½”ë“œ ì‹¤í–‰
    const analysisCode = `
import json
import numpy as np

# ë°ì´í„° ì¤€ë¹„
data = ${JSON.stringify(data)}

# ë¶„ì„ ì‹¤í–‰
if test_type == 'ttest':
    result = independent_ttest(data['group1'], data['group2'])
elif test_type == 'anova':
    result = one_way_anova(*data['groups'])
elif test_type == 'correlation':
    result = pearson_correlation(data['x'], data['y'])

# ê°€ì • ê²€ì •ë„ í•¨ê»˜ ì‹¤í–‰
assumptions = {
    'normality': normality_tests(data['group1']) if 'group1' in data else None,
    'homogeneity': homogeneity_tests(*data['groups']) if 'groups' in data else None
}

final_result = {
    'main_analysis': result,
    'assumptions': assumptions,
    'timestamp': str(datetime.now())
}

json.dumps(final_result, indent=2)
    `;
    
    const result = await pyodide.runPython(analysisCode);
    return JSON.parse(result);
}
```

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì‹œ í•„ìˆ˜ í™•ì¸ì‚¬í•­
- [ ] SciPy/statsmodels í•¨ìˆ˜ë§Œ ì‚¬ìš©
- [ ] JavaScriptë¡œ í†µê³„ ê³µì‹ ì§ì ‘ êµ¬í˜„ ê¸ˆì§€
- [ ] ëª¨ë“  ê°€ì • ê²€ì • í¬í•¨
- [ ] íš¨ê³¼í¬ê¸° ê³„ì‚° í¬í•¨
- [ ] ì‹ ë¢°êµ¬ê°„ ì œê³µ
- [ ] p-ê°’ í•´ì„ í¬í•¨
- [ ] ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„
- [ ] íƒ€ì… ì•ˆì „ì„± ë³´ì¥

---

*ìµœì¢… ì—…ë°ì´íŠ¸: 2025-09-12*  
*ë‹¤ìŒ ê²€í† : Phase 2 ì™„ë£Œ ì‹œ*
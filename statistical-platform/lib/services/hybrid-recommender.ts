/**
 * í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ
 * í‚¤ì›Œë“œ(ì¦‰ì‹œ) + LLM(ì •í™•) ì¡°í•©ìœ¼ë¡œ ìµœì ì˜ UX ì œê³µ
 */

import { KeywordBasedRecommender } from './keyword-based-recommender'
import { OllamaRecommender } from './ollama-recommender'
import { StatisticalMethod } from '@/types/smart-flow'

export interface HybridRecommendation {
  immediate: {
    methods: StatisticalMethod[]
    source: 'keyword'
    confidence: number
    timestamp: number
  }
  enhanced?: {
    methods: StatisticalMethod[]
    source: 'llm'
    confidence: number
    timestamp: number
    insights?: string
  }
  final: {
    methods: StatisticalMethod[]
    source: 'hybrid'
    confidence: number
    reasoning: string
  }
}

export class HybridRecommender {
  private keywordRecommender = KeywordBasedRecommender
  private ollamaRecommender = new OllamaRecommender()
  private cache = new Map<string, HybridRecommendation>()
  
  /**
   * ë©”ì¸ ì¶”ì²œ í•¨ìˆ˜ - ì ì§„ì  ê°œì„  ë°©ì‹
   */
  async recommend(
    purposeText: string,
    dataInfo: any,
    callbacks?: {
      onImmediate?: (result: any) => void
      onEnhanced?: (result: any) => void
      onFinal?: (result: any) => void
    }
  ): Promise<HybridRecommendation> {
    // ìºì‹œ í™•ì¸
    const cacheKey = `${purposeText}_${JSON.stringify(dataInfo)}`
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey)!
    }

    const result: HybridRecommendation = {
      immediate: {
        methods: [],
        source: 'keyword',
        confidence: 0,
        timestamp: Date.now()
      },
      final: {
        methods: [],
        source: 'hybrid',
        confidence: 0,
        reasoning: ''
      }
    }

    // ğŸš€ 1ë‹¨ê³„: ì¦‰ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œ (100ms ì´ë‚´)
    const keywordResult = this.getKeywordRecommendation(purposeText, dataInfo)
    result.immediate = {
      methods: keywordResult,
      source: 'keyword',
      confidence: this.calculateKeywordConfidence(purposeText, keywordResult),
      timestamp: Date.now()
    }
    
    // ì¦‰ì‹œ ì½œë°±
    callbacks?.onImmediate?.(result.immediate)

    // ğŸ¤– 2ë‹¨ê³„: LLM ë¶„ì„ (ë¹„ë™ê¸°, 2-5ì´ˆ)
    const llmPromise = this.getLLMRecommendation(purposeText, dataInfo)
      .then(llmResult => {
        result.enhanced = {
          methods: llmResult.methods,
          source: 'llm',
          confidence: llmResult.confidence,
          timestamp: Date.now(),
          insights: llmResult.insights
        }
        callbacks?.onEnhanced?.(result.enhanced)
        return llmResult
      })
      .catch(error => {
        console.error('LLM failed, using keyword only:', error)
        return null
      })

    // ğŸ¯ 3ë‹¨ê³„: ê²°ê³¼ í†µí•©
    const llmResult = await llmPromise
    
    if (llmResult) {
      // LLM ì„±ê³µ: ì§€ëŠ¥ì  í†µí•©
      result.final = this.mergeResults(
        result.immediate,
        result.enhanced!,
        dataInfo
      )
    } else {
      // LLM ì‹¤íŒ¨: í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©
      result.final = {
        methods: result.immediate.methods,
        source: 'hybrid',
        confidence: result.immediate.confidence * 0.8, // ì‹ ë¢°ë„ í•˜í–¥
        reasoning: 'ê¸°ë³¸ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ (ê³ ê¸‰ ë¶„ì„ ì‚¬ìš© ë¶ˆê°€)'
      }
    }

    // ìºì‹œ ì €ì¥
    this.cache.set(cacheKey, result)
    callbacks?.onFinal?.(result.final)

    return result
  }

  /**
   * í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œ (ì¦‰ì‹œ)
   */
  private getKeywordRecommendation(
    purposeText: string,
    dataInfo: any
  ): StatisticalMethod[] {
    return this.keywordRecommender.recommendMethods(purposeText, dataInfo)
  }

  /**
   * LLM ê¸°ë°˜ ì¶”ì²œ (ì •í™•)
   */
  private async getLLMRecommendation(
    purposeText: string,
    dataInfo: any
  ): Promise<any> {
    // Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    const isAvailable = await this.ollamaRecommender.checkHealth()
    
    if (!isAvailable) {
      throw new Error('Ollama not available')
    }

    return await this.ollamaRecommender.recommend(purposeText, {
      shape: [dataInfo.rowCount || 0, dataInfo.columnCount || 0],
      types: dataInfo.columnTypes || []
    })
  }

  /**
   * ê²°ê³¼ í†µí•© ì•Œê³ ë¦¬ì¦˜
   */
  private mergeResults(
    keywordResult: any,
    llmResult: any,
    dataInfo: any
  ): any {
    const merged: StatisticalMethod[] = []
    const reasoning: string[] = []

    // LLM ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ LLM ìš°ì„ 
    if (llmResult.confidence > 0.8) {
      merged.push(...llmResult.methods)
      reasoning.push('AI ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ')
    } 
    // ì¤‘ê°„ ì‹ ë¢°ë„ë©´ í†µí•©
    else if (llmResult.confidence > 0.5) {
      // LLM ìƒìœ„ 2ê°œ + í‚¤ì›Œë“œ ìƒìœ„ 1ê°œ
      merged.push(...llmResult.methods.slice(0, 2))
      merged.push(...keywordResult.methods.slice(0, 1))
      reasoning.push('AIì™€ í‚¤ì›Œë“œ ë¶„ì„ í†µí•©')
    }
    // ë‚®ì€ ì‹ ë¢°ë„ë©´ í‚¤ì›Œë“œ ìš°ì„ 
    else {
      merged.push(...keywordResult.methods)
      merged.push(...llmResult.methods.slice(0, 1))
      reasoning.push('í‚¤ì›Œë“œ ë¶„ì„ ìš°ì„  (AI ì‹ ë¢°ë„ ë‚®ìŒ)')
    }

    // ì¤‘ë³µ ì œê±°
    const unique = this.removeDuplicates(merged)

    // ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ë³´ì •
    const adjusted = this.adjustForDataCharacteristics(unique, dataInfo)

    return {
      methods: adjusted.slice(0, 4), // ìµœëŒ€ 4ê°œ
      source: 'hybrid',
      confidence: (keywordResult.confidence + llmResult.confidence) / 2,
      reasoning: reasoning.join('. ')
    }
  }

  /**
   * í‚¤ì›Œë“œ ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateKeywordConfidence(
    text: string,
    methods: StatisticalMethod[]
  ): number {
    let confidence = 0.5 // ê¸°ë³¸ê°’

    // í…ìŠ¤íŠ¸ ê¸¸ì´
    if (text.length > 20) confidence += 0.1
    if (text.length > 50) confidence += 0.1

    // ëª…í™•í•œ í‚¤ì›Œë“œ
    const clearKeywords = ['ë¹„êµ', 'ì°¨ì´', 'ê´€ê³„', 'ì˜í–¥', 'ì˜ˆì¸¡']
    const foundClear = clearKeywords.filter(k => text.includes(k)).length
    confidence += foundClear * 0.1

    // ì¶”ì²œ ê°œìˆ˜
    if (methods.length > 0) confidence += 0.1
    if (methods.length > 2) confidence += 0.1

    return Math.min(confidence, 0.9) // ìµœëŒ€ 0.9
  }

  /**
   * ì¤‘ë³µ ì œê±°
   */
  private removeDuplicates(methods: StatisticalMethod[]): StatisticalMethod[] {
    const seen = new Set<string>()
    return methods.filter(m => {
      if (seen.has(m.id)) return false
      seen.add(m.id)
      return true
    })
  }

  /**
   * ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ì¡°ì •
   */
  private adjustForDataCharacteristics(
    methods: StatisticalMethod[],
    dataInfo: any
  ): StatisticalMethod[] {
    const adjusted = [...methods]

    // ì‘ì€ ìƒ˜í”Œ â†’ ë¹„ëª¨ìˆ˜ ê²€ì • ìš°ì„ 
    if (dataInfo.rowCount < 30) {
      const nonparametric = {
        id: 'nonparametric-notice',
        name: 'âš ï¸ ë¹„ëª¨ìˆ˜ ê²€ì • ê¶Œì¥',
        description: `ìƒ˜í”Œì´ ì‘ìŠµë‹ˆë‹¤ (n=${dataInfo.rowCount})`,
        category: 'warning' as const
      }
      adjusted.unshift(nonparametric)
    }

    // ê²°ì¸¡ê°’ ë§ìŒ â†’ ê²½ê³ 
    if (dataInfo.missingRatio > 0.2) {
      const warning = {
        id: 'missing-data-notice',
        name: 'âš ï¸ ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìš”',
        description: `ê²°ì¸¡ê°’ ${(dataInfo.missingRatio * 100).toFixed(0)}%`,
        category: 'warning' as const
      }
      adjusted.unshift(warning)
    }

    return adjusted
  }

  /**
   * ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì²œ (ê³ ê¸‰ ê¸°ëŠ¥)
   */
  async *recommendStream(
    purposeText: string,
    dataInfo: any
  ): AsyncGenerator<Partial<HybridRecommendation>> {
    // 1. ì¦‰ì‹œ í‚¤ì›Œë“œ ê²°ê³¼
    const keywordResult = this.getKeywordRecommendation(purposeText, dataInfo)
    yield {
      immediate: {
        methods: keywordResult,
        source: 'keyword',
        confidence: 0.6,
        timestamp: Date.now()
      }
    }

    // 2. LLM ìŠ¤íŠ¸ë¦¬ë°
    try {
      const stream = this.ollamaRecommender.recommendStream(purposeText, dataInfo)
      let accumulated = ''
      
      for await (const chunk of stream) {
        accumulated += chunk
        // ë¶€ë¶„ ê²°ê³¼ ì „ì†¡
        yield {
          enhanced: {
            methods: [],
            source: 'llm',
            confidence: 0,
            timestamp: Date.now(),
            insights: accumulated
          }
        }
      }
    } catch (error) {
      console.error('LLM streaming failed:', error)
    }

    // 3. ìµœì¢… í†µí•© ê²°ê³¼
    // ... í†µí•© ë¡œì§
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const hybridRecommender = new HybridRecommender()
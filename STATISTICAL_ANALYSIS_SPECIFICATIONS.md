# ğŸ“Š Statistical Analysis Specifications

## ê°œìš”

ì´ ë¬¸ì„œëŠ” í†µê³„ ë¶„ì„ í”Œë«í¼ì—ì„œ êµ¬í˜„í•  **ëª¨ë“  í†µê³„ì  ê¸°ëŠ¥ì˜ ìƒì„¸ ëª…ì„¸**ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ê° ë¶„ì„ ë°©ë²•ì€ **ì…ë ¥ ìš”êµ¬ì‚¬í•­, ê°€ì • ì¡°ê±´, ì¶œë ¥ í˜•ì‹, í•´ì„ ê°€ì´ë“œë¼ì¸**ì„ í¬í•¨í•©ë‹ˆë‹¤.

## 1. ê¸°ìˆ í†µê³„ (Descriptive Statistics)

### 1.1 ì¤‘ì‹¬ê²½í–¥ì„± ì¸¡ë„

#### êµ¬í˜„ í•¨ìˆ˜
```python
def descriptive_statistics(data: np.ndarray) -> Dict[str, Any]:
    """
    ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚°
    
    Parameters:
    -----------
    data : np.ndarray
        ë¶„ì„ ëŒ€ìƒ ë°ì´í„° (1ì°¨ì› ë°°ì—´)
    
    Returns:
    --------
    dict : ê¸°ìˆ í†µê³„ëŸ‰ ë”•ì…”ë„ˆë¦¬
    """
```

#### ì¶œë ¥ ëª…ì„¸
```json
{
  "central_tendency": {
    "mean": 2.456,
    "median": 2.400,
    "mode": [2.1, 2.3],  // ìµœë¹ˆê°’ (ë‹¤ì¤‘ê°’ ê°€ëŠ¥)
    "geometric_mean": 2.234,  // ê¸°í•˜í‰ê· 
    "harmonic_mean": 2.123    // ì¡°í™”í‰ê· 
  },
  "variability": {
    "variance": 1.234,
    "std_dev": 1.111,
    "std_error": 0.156,
    "range": 4.2,
    "iqr": 1.5,           // ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„
    "mad": 0.89,          // ì¤‘ì•™ì ˆëŒ€í¸ì°¨
    "cv": 0.452           // ë³€ì´ê³„ìˆ˜
  },
  "distribution_shape": {
    "skewness": -0.234,
    "kurtosis": 0.567,
    "skewness_interpretation": "approximately symmetric",
    "kurtosis_interpretation": "platykurtic"
  },
  "quartiles": {
    "q1": 1.8,
    "q2": 2.4,  // median
    "q3": 3.3
  },
  "percentiles": {
    "p5": 1.2,
    "p10": 1.5,
    "p90": 4.1,
    "p95": 4.3
  },
  "confidence_intervals": {
    "mean_95": [2.145, 2.767],
    "mean_99": [2.089, 2.823]
  },
  "sample_info": {
    "n": 120,
    "missing_values": 3,
    "outliers": [4.8, 5.2]  // IQR ê¸°ì¤€ ì´ìƒê°’
  }
}
```

### 1.2 ë¶„í¬ í˜•íƒœ ë¶„ì„

#### ì •ê·œì„± í•´ì„ ê¸°ì¤€
```python
def interpret_normality(skewness: float, kurtosis: float) -> Dict[str, str]:
    """
    ì™œë„ì™€ ì²¨ë„ í•´ì„
    
    ì™œë„ í•´ì„:
    - |skewness| < 0.5: approximately symmetric
    - 0.5 â‰¤ |skewness| < 1.0: moderately skewed
    - |skewness| â‰¥ 1.0: highly skewed
    
    ì²¨ë„ í•´ì„:
    - kurtosis < -1: platykurtic (í‰í‰í•œ ë¶„í¬)
    - -1 â‰¤ kurtosis â‰¤ 1: mesokurtic (ì •ê·œë¶„í¬ ìˆ˜ì¤€)
    - kurtosis > 1: leptokurtic (ë¾°ì¡±í•œ ë¶„í¬)
    """
```

## 2. ê°€ì • ê²€ì • (Assumption Testing)

### 2.1 ì •ê·œì„± ê²€ì •

#### Shapiro-Wilk Test (n â‰¤ 50)
```python
def shapiro_wilk_test(data: np.ndarray) -> Dict[str, Any]:
    """
    Shapiro-Wilk ì •ê·œì„± ê²€ì •
    
    ì ìš© ì¡°ê±´:
    - í‘œë³¸ í¬ê¸°: 3 â‰¤ n â‰¤ 50
    - ì—°ì†í˜• ë°ì´í„°
    
    ê°€ì„¤:
    - Hâ‚€: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤
    - Hâ‚: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤
    """
```

#### Kolmogorov-Smirnov Test (n > 50)
```python
def kolmogorov_smirnov_test(data: np.ndarray) -> Dict[str, Any]:
    """
    Kolmogorov-Smirnov ì •ê·œì„± ê²€ì •
    
    ì ìš© ì¡°ê±´:
    - í‘œë³¸ í¬ê¸°: n > 50
    - ì—°ì†í˜• ë°ì´í„°
    
    ê°€ì„¤:
    - Hâ‚€: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤
    - Hâ‚: ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤
    """
```

#### ì¶œë ¥ ëª…ì„¸
```json
{
  "test_name": "Shapiro-Wilk",
  "statistic": 0.987,
  "p_value": 0.234,
  "alpha": 0.05,
  "result": {
    "is_normal": true,
    "interpretation": "Fail to reject Hâ‚€. Data appears to be normally distributed.",
    "recommendation": "Proceed with parametric tests."
  },
  "alternative_tests": {
    "anderson_darling": {
      "statistic": 0.456,
      "critical_values": [0.576, 0.656, 0.787, 0.918, 1.092],
      "significance_levels": [0.15, 0.10, 0.05, 0.025, 0.01]
    },
    "dagostino_pearson": {
      "statistic": 2.34,
      "p_value": 0.310
    }
  },
  "visual_checks": {
    "qq_plot_correlation": 0.992,  // Q-Q plot ìƒê´€ê³„ìˆ˜
    "histogram_assessment": "bell-shaped",
    "recommendations": [
      "Q-Q plot shows good linear relationship",
      "Histogram appears approximately bell-shaped"
    ]
  }
}
```

### 2.2 ë“±ë¶„ì‚°ì„± ê²€ì •

#### Levene's Test
```python
def levene_test(*groups) -> Dict[str, Any]:
    """
    Leveneì˜ ë“±ë¶„ì‚°ì„± ê²€ì •
    
    íŠ¹ì§•:
    - ì •ê·œë¶„í¬ ê°€ì •ì´ ë¶ˆí•„ìš”
    - ì¤‘ì•™ê°’ ê¸°ì¤€ ê³„ì‚°ìœ¼ë¡œ ë¡œë²„ìŠ¤íŠ¸
    
    ê°€ì„¤:
    - Hâ‚€: ëª¨ë“  ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ê°™ë‹¤
    - Hâ‚: ì ì–´ë„ í•œ ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ë‹¤ë¥´ë‹¤
    """
```

#### Bartlett's Test
```python
def bartlett_test(*groups) -> Dict[str, Any]:
    """
    Bartlettì˜ ë“±ë¶„ì‚°ì„± ê²€ì •
    
    íŠ¹ì§•:
    - ì •ê·œë¶„í¬ ê°€ì • í•„ìš”
    - ì •ê·œë¶„í¬ì¼ ë•Œ ë” ê²€ì •ë ¥ì´ ë†’ìŒ
    
    ê°€ì„¤:
    - Hâ‚€: ëª¨ë“  ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ê°™ë‹¤
    - Hâ‚: ì ì–´ë„ í•œ ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ë‹¤ë¥´ë‹¤
    """
```

#### ì¶œë ¥ ëª…ì„¸
```json
{
  "levene_test": {
    "statistic": 1.234,
    "p_value": 0.345,
    "df_between": 2,
    "df_within": 117,
    "result": {
      "is_homogeneous": true,
      "interpretation": "Variances appear to be equal across groups."
    }
  },
  "bartlett_test": {
    "statistic": 2.456,
    "p_value": 0.293,
    "df": 2,
    "result": {
      "is_homogeneous": true,
      "interpretation": "Variances are equal (assuming normality)."
    }
  },
  "group_variances": [
    {"group": "Group 1", "n": 40, "variance": 1.23, "std_dev": 1.11},
    {"group": "Group 2", "n": 40, "variance": 1.45, "std_dev": 1.20},
    {"group": "Group 3", "n": 40, "variance": 1.18, "std_dev": 1.09}
  ],
  "variance_ratios": {
    "max_to_min": 1.23,  // ìµœëŒ€ë¶„ì‚°/ìµœì†Œë¶„ì‚°
    "interpretation": "Variance ratios are within acceptable range (<4:1)"
  },
  "recommendation": {
    "test_to_use": "parametric",
    "reason": "Both normality and homogeneity assumptions are met",
    "suggested_analysis": "One-way ANOVA"
  }
}
```

### 2.3 ë…ë¦½ì„± ê²€ì •

#### Durbin-Watson Test
```python
def durbin_watson_test(residuals: np.ndarray) -> Dict[str, Any]:
    """
    Durbin-Watson ë…ë¦½ì„± ê²€ì • (ìê¸°ìƒê´€ ê²€ì •)
    
    ì ìš©:
    - íšŒê·€ë¶„ì„ ì”ì°¨ì˜ ìê¸°ìƒê´€ ê²€ì •
    - ì‹œê³„ì—´ ë°ì´í„°ì˜ ë…ë¦½ì„± ê²€ì •
    
    í•´ì„:
    - DW â‰ˆ 2: ìê¸°ìƒê´€ ì—†ìŒ
    - DW < 2: ì–‘ì˜ ìê¸°ìƒê´€
    - DW > 2: ìŒì˜ ìê¸°ìƒê´€
    """
```

## 3. ì¶”ë¡ í†µê³„ (Inferential Statistics)

### 3.1 t-ê²€ì • (t-tests)

#### ë‹¨ì¼í‘œë³¸ t-ê²€ì •
```python
def one_sample_ttest(data: np.ndarray, mu0: float, alpha: float = 0.05) -> Dict[str, Any]:
    """
    ë‹¨ì¼í‘œë³¸ t-ê²€ì •
    
    ê°€ì„¤:
    - Hâ‚€: Î¼ = Î¼â‚€
    - Hâ‚: Î¼ â‰  Î¼â‚€ (ì–‘ì¸¡ê²€ì •)
    
    ê°€ì •:
    - ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (ë˜ëŠ” n â‰¥ 30)
    - ê´€ì¸¡ê°’ë“¤ì´ ë…ë¦½ì 
    """
```

#### ë…ë¦½í‘œë³¸ t-ê²€ì •
```python
def independent_ttest(group1: np.ndarray, group2: np.ndarray, 
                     equal_var: bool = True, alpha: float = 0.05) -> Dict[str, Any]:
    """
    ë…ë¦½í‘œë³¸ t-ê²€ì •
    
    ê°€ì„¤:
    - Hâ‚€: Î¼â‚ = Î¼â‚‚
    - Hâ‚: Î¼â‚ â‰  Î¼â‚‚
    
    ê°€ì •:
    - ë‘ ê·¸ë£¹ ëª¨ë‘ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„
    - ê´€ì¸¡ê°’ë“¤ì´ ë…ë¦½ì 
    - equal_var=True: ë“±ë¶„ì‚°ì„± ê°€ì •
    - equal_var=False: Welchì˜ t-ê²€ì • (ì´ë¶„ì‚°ì„± í—ˆìš©)
    """
```

#### ëŒ€ì‘í‘œë³¸ t-ê²€ì •
```python
def paired_ttest(before: np.ndarray, after: np.ndarray, alpha: float = 0.05) -> Dict[str, Any]:
    """
    ëŒ€ì‘í‘œë³¸ t-ê²€ì •
    
    ê°€ì„¤:
    - Hâ‚€: Î¼d = 0 (ì°¨ì´ì˜ í‰ê· ì´ 0)
    - Hâ‚: Î¼d â‰  0
    
    ê°€ì •:
    - ì°¨ì´ê°’(d)ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„
    - ìŒë“¤ì´ ë…ë¦½ì 
    """
```

#### t-ê²€ì • ì¶œë ¥ ëª…ì„¸
```json
{
  "test_info": {
    "test_type": "Independent t-test",
    "hypothesis": {
      "null": "Î¼â‚ = Î¼â‚‚",
      "alternative": "Î¼â‚ â‰  Î¼â‚‚",
      "test_type": "two-tailed"
    },
    "assumptions": {
      "normality": true,
      "independence": true,
      "equal_variances": true
    }
  },
  "descriptives": {
    "group1": {
      "name": "Control",
      "n": 45,
      "mean": 2.34,
      "std_dev": 0.87,
      "std_error": 0.13,
      "ci_95": [2.08, 2.60]
    },
    "group2": {
      "name": "Treatment",
      "n": 43,
      "mean": 2.89,
      "std_dev": 0.94,
      "std_error": 0.14,
      "ci_95": [2.60, 3.18]
    }
  },
  "test_statistics": {
    "t_statistic": -2.876,
    "degrees_of_freedom": 86,
    "p_value": 0.005,
    "p_value_one_tailed": 0.0025,
    "alpha": 0.05,
    "critical_value": 1.988
  },
  "effect_size": {
    "cohens_d": 0.613,
    "interpretation": "medium effect",
    "pooled_std": 0.904,
    "confidence_interval": [0.185, 1.040]
  },
  "results": {
    "significant": true,
    "interpretation": "Reject Hâ‚€. There is significant difference between groups.",
    "conclusion": "Treatment group shows significantly higher scores than control group.",
    "mean_difference": 0.55,
    "mean_difference_ci": [0.17, 0.93]
  },
  "assumptions_check": {
    "normality": {
      "group1_shapiro_p": 0.234,
      "group2_shapiro_p": 0.456,
      "assumption_met": true
    },
    "equal_variances": {
      "levene_p": 0.567,
      "assumption_met": true
    }
  },
  "power_analysis": {
    "observed_power": 0.832,
    "required_n_per_group": 34,
    "interpretation": "Study has adequate power (>0.80)"
  }
}
```

### 3.2 ë¶„ì‚°ë¶„ì„ (ANOVA)

#### ì¼ì›ë¶„ì‚°ë¶„ì„ (One-way ANOVA)
```python
def one_way_anova(*groups) -> Dict[str, Any]:
    """
    ì¼ì›ë¶„ì‚°ë¶„ì„
    
    ê°€ì„¤:
    - Hâ‚€: Î¼â‚ = Î¼â‚‚ = ... = Î¼â‚– (ëª¨ë“  ê·¸ë£¹ í‰ê· ì´ ê°™ìŒ)
    - Hâ‚: ì ì–´ë„ í•œ ê·¸ë£¹ì˜ í‰ê· ì´ ë‹¤ë¦„
    
    ê°€ì •:
    - ê° ê·¸ë£¹ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„
    - ë“±ë¶„ì‚°ì„± (homogeneity of variances)
    - ê´€ì¸¡ê°’ë“¤ì˜ ë…ë¦½ì„±
    """
```

#### ì´ì›ë¶„ì‚°ë¶„ì„ (Two-way ANOVA)
```python
def two_way_anova(data: pd.DataFrame, dependent_var: str, 
                  factor1: str, factor2: str, interaction: bool = True) -> Dict[str, Any]:
    """
    ì´ì›ë¶„ì‚°ë¶„ì„
    
    ê°€ì„¤:
    - ì£¼íš¨ê³¼ A: Hâ‚€: ëª¨ë“  A ìˆ˜ì¤€ì˜ í‰ê· ì´ ê°™ìŒ
    - ì£¼íš¨ê³¼ B: Hâ‚€: ëª¨ë“  B ìˆ˜ì¤€ì˜ í‰ê· ì´ ê°™ìŒ  
    - ìƒí˜¸ì‘ìš©: Hâ‚€: Aì™€ B ê°„ ìƒí˜¸ì‘ìš©ì´ ì—†ìŒ
    
    ê°€ì •:
    - ì •ê·œì„±, ë“±ë¶„ì‚°ì„±, ë…ë¦½ì„±
    - ì…€ë³„ ì¶©ë¶„í•œ í‘œë³¸ í¬ê¸°
    """
```

#### ANOVA ì¶œë ¥ ëª…ì„¸
```json
{
  "test_info": {
    "test_type": "One-way ANOVA",
    "hypothesis": {
      "null": "Î¼â‚ = Î¼â‚‚ = Î¼â‚ƒ",
      "alternative": "At least one group mean differs"
    },
    "groups": ["Control", "Treatment A", "Treatment B"],
    "total_n": 120
  },
  "descriptives": [
    {
      "group": "Control",
      "n": 40,
      "mean": 2.34,
      "std_dev": 0.87,
      "std_error": 0.138,
      "ci_95": [2.06, 2.62]
    },
    {
      "group": "Treatment A", 
      "n": 40,
      "mean": 2.89,
      "std_dev": 0.94,
      "std_error": 0.149,
      "ci_95": [2.59, 3.19]
    },
    {
      "group": "Treatment B",
      "n": 40,
      "mean": 3.45,
      "std_dev": 1.02,
      "std_error": 0.161,
      "ci_95": [3.12, 3.78]
    }
  ],
  "anova_table": {
    "between_groups": {
      "sum_of_squares": 25.67,
      "degrees_of_freedom": 2,
      "mean_square": 12.835,
      "f_statistic": 14.58,
      "p_value": 0.000012
    },
    "within_groups": {
      "sum_of_squares": 103.24,
      "degrees_of_freedom": 117,
      "mean_square": 0.882
    },
    "total": {
      "sum_of_squares": 128.91,
      "degrees_of_freedom": 119
    }
  },
  "effect_size": {
    "eta_squared": 0.199,
    "interpretation": "large effect",
    "omega_squared": 0.185,
    "partial_eta_squared": 0.199
  },
  "results": {
    "significant": true,
    "interpretation": "Reject Hâ‚€. There are significant differences between groups.",
    "f_critical": 3.07,
    "power": 0.998
  },
  "assumptions_check": {
    "normality": {
      "shapiro_results": [0.234, 0.456, 0.123],
      "assumption_met": true
    },
    "homogeneity": {
      "levene_p": 0.234,
      "bartlett_p": 0.345,
      "assumption_met": true
    }
  },
  "post_hoc": {
    "required": true,
    "recommended_test": "Tukey HSD",
    "reason": "Equal variances assumed, equal sample sizes"
  }
}
```

### 3.3 ì‚¬í›„ë¶„ì„ (Post-hoc Analysis)

#### Tukey HSD (ì •ì§í•œ ìœ ì˜ì°¨ ê²€ì •)
```python
def tukey_hsd(*groups, group_names: List[str] = None, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Tukey HSD ì‚¬í›„ë¶„ì„
    
    ì ìš© ì¡°ê±´:
    - ANOVAì—ì„œ ìœ ì˜í•œ ê²°ê³¼
    - ë“±ë¶„ì‚°ì„± ê°€ì • ë§Œì¡±
    - ëª¨ë“  ìŒë³„ ë¹„êµ
    
    íŠ¹ì§•:
    - Type I Errorë¥¼ Î± ìˆ˜ì¤€ìœ¼ë¡œ ì œì–´
    - ë³´ìˆ˜ì  ì ‘ê·¼ (ë‚®ì€ ê²€ì •ë ¥)
    """
```

#### Games-Howell Test
```python
def games_howell_test(*groups, group_names: List[str] = None, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Games-Howell ì‚¬í›„ë¶„ì„
    
    ì ìš© ì¡°ê±´:
    - ANOVAì—ì„œ ìœ ì˜í•œ ê²°ê³¼
    - ë“±ë¶„ì‚°ì„± ê°€ì • ìœ„ë°˜
    - í‘œë³¸ í¬ê¸°ê°€ ë‹¤ë¥¼ ë•Œ ì í•©
    
    íŠ¹ì§•:
    - Welchì˜ t-ê²€ì • ê¸°ë°˜
    - ì´ë¶„ì‚°ì„±ì— ë¡œë²„ìŠ¤íŠ¸
    """
```

#### Dunn's Test (ë¹„ëª¨ìˆ˜)
```python
def dunn_test(*groups, group_names: List[str] = None, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Dunn's test ì‚¬í›„ë¶„ì„
    
    ì ìš© ì¡°ê±´:
    - Kruskal-Wallis ê²€ì •ì—ì„œ ìœ ì˜í•œ ê²°ê³¼
    - ë¹„ëª¨ìˆ˜ ì‚¬í›„ë¶„ì„
    
    íŠ¹ì§•:
    - ìˆœìœ„ ê¸°ë°˜ ê²€ì •
    - ì •ê·œë¶„í¬ ê°€ì • ë¶ˆí•„ìš”
    """
```

#### ì‚¬í›„ë¶„ì„ ì¶œë ¥ ëª…ì„¸
```json
{
  "test_info": {
    "method": "Tukey HSD",
    "family_wise_error_rate": 0.05,
    "total_comparisons": 3,
    "adjustment": "Studentized Range Distribution"
  },
  "pairwise_comparisons": [
    {
      "comparison": "Control vs Treatment A",
      "group1_mean": 2.34,
      "group2_mean": 2.89,
      "mean_difference": -0.55,
      "std_error": 0.148,
      "q_statistic": 3.716,
      "p_value": 0.0234,
      "p_value_adjusted": 0.0234,
      "ci_95": [-0.932, -0.168],
      "significant": true,
      "interpretation": "Treatment A significantly higher than Control"
    },
    {
      "comparison": "Control vs Treatment B", 
      "group1_mean": 2.34,
      "group2_mean": 3.45,
      "mean_difference": -1.11,
      "std_error": 0.148,
      "q_statistic": 7.499,
      "p_value": 0.0001,
      "p_value_adjusted": 0.0001,
      "ci_95": [-1.492, -0.728],
      "significant": true,
      "interpretation": "Treatment B significantly higher than Control"
    },
    {
      "comparison": "Treatment A vs Treatment B",
      "group1_mean": 2.89,
      "group2_mean": 3.45,
      "mean_difference": -0.56,
      "std_error": 0.148,
      "q_statistic": 3.783,
      "p_value": 0.0214,
      "p_value_adjusted": 0.0214,
      "ci_95": [-0.942, -0.178],
      "significant": true,
      "interpretation": "Treatment B significantly higher than Treatment A"
    }
  ],
  "summary": {
    "significant_pairs": 3,
    "non_significant_pairs": 0,
    "homogeneous_subsets": [
      {"subset": 1, "groups": ["Control"], "mean": 2.34},
      {"subset": 2, "groups": ["Treatment A"], "mean": 2.89},
      {"subset": 3, "groups": ["Treatment B"], "mean": 3.45}
    ]
  },
  "multiple_comparisons_summary": {
    "method_used": "Tukey HSD",
    "alpha": 0.05,
    "critical_value": 3.37,
    "conclusions": [
      "All groups differ significantly from each other",
      "Treatment B > Treatment A > Control"
    ]
  }
}
```

### 3.4 ë¹„ëª¨ìˆ˜ ê²€ì • (Non-parametric Tests)

#### Mann-Whitney U Test
```python
def mann_whitney_u_test(group1: np.ndarray, group2: np.ndarray, 
                       alternative: str = 'two-sided') -> Dict[str, Any]:
    """
    Mann-Whitney U ê²€ì • (Wilcoxon rank-sum test)
    
    ìš©ë„:
    - ë‘ ë…ë¦½ ê·¸ë£¹ì˜ ì¤‘ì•™ê°’ ë¹„êµ
    - t-ê²€ì •ì˜ ë¹„ëª¨ìˆ˜ ëŒ€ì•ˆ
    
    ê°€ì •:
    - ì—°ì†í˜• ë˜ëŠ” ì„œì—´í˜• ë°ì´í„°
    - ë‘ ê·¸ë£¹ì˜ ë…ë¦½ì„±
    - ë‘ ê·¸ë£¹ì˜ ë¶„í¬ í˜•íƒœê°€ ìœ ì‚¬
    """
```

#### Kruskal-Wallis Test
```python
def kruskal_wallis_test(*groups) -> Dict[str, Any]:
    """
    Kruskal-Wallis H ê²€ì •
    
    ìš©ë„:
    - 3ê°œ ì´ìƒ ë…ë¦½ ê·¸ë£¹ì˜ ì¤‘ì•™ê°’ ë¹„êµ
    - ì¼ì›ë¶„ì‚°ë¶„ì„ì˜ ë¹„ëª¨ìˆ˜ ëŒ€ì•ˆ
    
    ê°€ì •:
    - ì—°ì†í˜• ë˜ëŠ” ì„œì—´í˜• ë°ì´í„°
    - ê·¸ë£¹ ê°„ ë…ë¦½ì„±
    - ê° ê·¸ë£¹ì˜ ë¶„í¬ í˜•íƒœê°€ ìœ ì‚¬
    """
```

#### Wilcoxon Signed-Rank Test
```python
def wilcoxon_signed_rank_test(before: np.ndarray, after: np.ndarray) -> Dict[str, Any]:
    """
    Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì •
    
    ìš©ë„:
    - ëŒ€ì‘í‘œë³¸ì˜ ì¤‘ì•™ê°’ ì°¨ì´ ê²€ì •
    - ëŒ€ì‘í‘œë³¸ t-ê²€ì •ì˜ ë¹„ëª¨ìˆ˜ ëŒ€ì•ˆ
    
    ê°€ì •:
    - ì°¨ì´ê°’ë“¤ì´ ì—°ì†í˜•
    - ì°¨ì´ê°’ë“¤ì´ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ë¶„í¬
    """
```

#### ë¹„ëª¨ìˆ˜ ê²€ì • ì¶œë ¥ ëª…ì„¸
```json
{
  "test_info": {
    "test_type": "Mann-Whitney U Test",
    "hypothesis": {
      "null": "ë‘ ê·¸ë£¹ì˜ ë¶„í¬ê°€ ê°™ë‹¤",
      "alternative": "ë‘ ê·¸ë£¹ì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤"
    },
    "alternative_hypothesis": "two-sided"
  },
  "descriptives": {
    "group1": {
      "n": 45,
      "median": 2.1,
      "iqr": [1.8, 2.6],
      "mean_rank": 38.2
    },
    "group2": {
      "n": 43,
      "median": 2.7,
      "iqr": [2.3, 3.1],
      "mean_rank": 51.1
    }
  },
  "test_statistics": {
    "u_statistic": 642,
    "u_critical": 736,
    "z_statistic": -2.456,
    "p_value": 0.014,
    "ties_correction": true,
    "continuity_correction": true
  },
  "effect_size": {
    "rank_biserial_correlation": 0.343,
    "interpretation": "medium effect",
    "probability_superiority": 0.671,
    "common_language_effect": "Treatment group scores higher 67.1% of the time"
  },
  "results": {
    "significant": true,
    "interpretation": "Reject Hâ‚€. ë‘ ê·¸ë£¹ì˜ ë¶„í¬ê°€ ìœ ì˜í•˜ê²Œ ë‹¤ë¥´ë‹¤.",
    "conclusion": "Treatment group shows significantly higher values than control group.",
    "median_difference": 0.6
  },
  "diagnostic_info": {
    "total_observations": 88,
    "total_ties": 12,
    "large_sample_approximation": true,
    "exact_test_available": false
  }
}
```

### 3.5 íšŒê·€ë¶„ì„ (Regression Analysis)

#### ë‹¨ìˆœì„ í˜•íšŒê·€
```python
def simple_linear_regression(x: np.ndarray, y: np.ndarray, alpha: float = 0.05) -> Dict[str, Any]:
    """
    ë‹¨ìˆœì„ í˜•íšŒê·€ë¶„ì„
    
    ëª¨ë¸: y = Î²â‚€ + Î²â‚x + Îµ
    
    ê°€ì •:
    - ì„ í˜•ê´€ê³„ (Linearity)
    - ë…ë¦½ì„± (Independence)
    - ì •ê·œë¶„í¬ (Normality) of residuals
    - ë“±ë¶„ì‚°ì„± (Homoscedasticity)
    """
```

#### ë‹¤ì¤‘ì„ í˜•íšŒê·€
```python
def multiple_linear_regression(X: np.ndarray, y: np.ndarray, 
                              feature_names: List[str] = None,
                              alpha: float = 0.05) -> Dict[str, Any]:
    """
    ë‹¤ì¤‘ì„ í˜•íšŒê·€ë¶„ì„
    
    ëª¨ë¸: y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚šxâ‚š + Îµ
    
    ì¶”ê°€ ê³ ë ¤ì‚¬í•­:
    - ë‹¤ì¤‘ê³µì„ ì„± (Multicollinearity)
    - ë³€ìˆ˜ ì„ íƒ (Variable Selection)
    """
```

#### íšŒê·€ë¶„ì„ ì¶œë ¥ ëª…ì„¸
```json
{
  "model_info": {
    "model_type": "Simple Linear Regression",
    "equation": "y = Î²â‚€ + Î²â‚x + Îµ",
    "sample_size": 100,
    "degrees_of_freedom": {
      "regression": 1,
      "residual": 98,
      "total": 99
    }
  },
  "model_fit": {
    "r_squared": 0.847,
    "adjusted_r_squared": 0.845,
    "standard_error": 0.234,
    "f_statistic": 542.3,
    "f_p_value": 0.000001,
    "aic": -123.45,
    "bic": -118.23
  },
  "coefficients": [
    {
      "variable": "Intercept",
      "coefficient": 0.456,
      "std_error": 0.123,
      "t_statistic": 3.707,
      "p_value": 0.0004,
      "ci_95": [0.212, 0.700],
      "significant": true
    },
    {
      "variable": "x", 
      "coefficient": 1.234,
      "std_error": 0.053,
      "t_statistic": 23.28,
      "p_value": 0.000001,
      "ci_95": [1.129, 1.339],
      "significant": true,
      "standardized_coefficient": 0.921
    }
  ],
  "residual_analysis": {
    "normality": {
      "shapiro_p": 0.234,
      "assumption_met": true
    },
    "homoscedasticity": {
      "breusch_pagan_p": 0.456,
      "assumption_met": true
    },
    "autocorrelation": {
      "durbin_watson": 1.987,
      "interpretation": "No autocorrelation"
    },
    "outliers": {
      "studentized_residuals": [-2.1, 2.3],
      "cooks_distance": [0.034, 0.067],
      "leverage": [0.023, 0.045]
    }
  },
  "predictions": {
    "fitted_values_range": [1.23, 4.56],
    "residuals_range": [-0.67, 0.89],
    "prediction_intervals": true,
    "confidence_intervals": true
  }
}
```

### 3.6 ìƒê´€ë¶„ì„ (Correlation Analysis)

#### Pearson ìƒê´€ê³„ìˆ˜
```python
def pearson_correlation(x: np.ndarray, y: np.ndarray, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Pearson ê³±-ëª¨ë©˜íŠ¸ ìƒê´€ê³„ìˆ˜
    
    ê°€ì •:
    - ë‘ ë³€ìˆ˜ ëª¨ë‘ ì—°ì†í˜•
    - ë‘ ë³€ìˆ˜ ëª¨ë‘ ì •ê·œë¶„í¬
    - ì„ í˜•ê´€ê³„
    - ì´ìƒê°’ì˜ ì˜í–¥ì„ ë°›ê¸° ì‰¬ì›€
    """
```

#### Spearman ìˆœìœ„ìƒê´€ê³„ìˆ˜
```python
def spearman_correlation(x: np.ndarray, y: np.ndarray, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Spearman ìˆœìœ„ìƒê´€ê³„ìˆ˜
    
    íŠ¹ì§•:
    - ë¹„ëª¨ìˆ˜ ìƒê´€ê³„ìˆ˜
    - ë‹¨ì¡°ê´€ê³„ ì¸¡ì •
    - ì´ìƒê°’ì— ë¡œë²„ìŠ¤íŠ¸
    - ì„œì—´í˜• ë°ì´í„° ì ìš© ê°€ëŠ¥
    """
```

#### ìƒê´€ë¶„ì„ ì¶œë ¥ ëª…ì„¸
```json
{
  "correlation_matrix": {
    "pearson": [
      [1.000, 0.847, -0.234],
      [0.847, 1.000, -0.156],
      [-0.234, -0.156, 1.000]
    ],
    "spearman": [
      [1.000, 0.823, -0.267],
      [0.823, 1.000, -0.178],
      [-0.267, -0.178, 1.000]
    ]
  },
  "significance_tests": [
    {
      "variables": "X1 vs X2",
      "pearson_r": 0.847,
      "pearson_p": 0.000001,
      "spearman_rho": 0.823,
      "spearman_p": 0.000002,
      "sample_size": 100,
      "degrees_of_freedom": 98,
      "confidence_interval": [0.789, 0.892]
    }
  ],
  "interpretations": [
    {
      "correlation": 0.847,
      "strength": "strong positive",
      "interpretation": "As X1 increases, X2 tends to increase substantially",
      "effect_size": "large effect",
      "shared_variance": 71.7  // rÂ² Ã— 100%
    }
  ]
}
```

## 4. ê³ ê¸‰ ë¶„ì„ (Advanced Analysis)

### 4.1 ê²€ì •ë ¥ ë¶„ì„ (Power Analysis)

```python
def power_analysis(effect_size: float, alpha: float = 0.05, 
                  power: float = 0.80, test_type: str = 'ttest') -> Dict[str, Any]:
    """
    ê²€ì •ë ¥ ë¶„ì„
    
    ê³„ì‚° ê°€ëŠ¥í•œ ê°’:
    - í‘œë³¸ í¬ê¸° (given effect size, alpha, power)
    - ê²€ì •ë ¥ (given effect size, alpha, sample size)
    - íš¨ê³¼í¬ê¸° (given alpha, power, sample size)
    """
```

### 4.2 ë©”íƒ€ë¶„ì„ ê¸°ì´ˆ (Meta-analysis Basics)

```python
def effect_size_calculation(studies: List[Dict]) -> Dict[str, Any]:
    """
    íš¨ê³¼í¬ê¸° í†µí•© ë¶„ì„
    
    ì§€ì›í•˜ëŠ” íš¨ê³¼í¬ê¸°:
    - Cohen's d
    - Hedges' g
    - Correlation coefficients
    - Odds ratios
    """
```

## 5. ìˆ˜ì‚°ê³¼í•™ íŠ¹í™” ë¶„ì„

### 5.1 CPUE ë¶„ì„ (Catch Per Unit Effort)

```python
def cpue_analysis(catch_data: pd.DataFrame, effort_data: pd.DataFrame) -> Dict[str, Any]:
    """
    ì–´íšëŸ‰ ë‹¨ìœ„ë…¸ë ¥ ë¶„ì„
    
    ê³„ì‚° ì§€í‘œ:
    - ê¸°ë³¸ CPUE (ì–´íšëŸ‰/ë…¸ë ¥ëŸ‰)
    - í‘œì¤€í™” CPUE
    - ì‹œê³„ì—´ ì¶”ì„¸ ë¶„ì„
    - ê³µê°„ì  ë¶„í¬ ë¶„ì„
    """
```

### 5.2 ì„±ì¥ ëª¨ë¸ ë¶„ì„

```python
def von_bertalanffy_growth(age: np.ndarray, length: np.ndarray) -> Dict[str, Any]:
    """
    von Bertalanffy ì„±ì¥ ëª¨ë¸
    
    ëª¨ë¸: L(t) = Lâˆ(1 - e^(-K(t-tâ‚€)))
    
    ë§¤ê°œë³€ìˆ˜:
    - Lâˆ: ê·¹í•œì „ì¥
    - K: ì„±ì¥ê³„ìˆ˜  
    - tâ‚€: ì´ë¡ ì  ì—°ë ¹ 0ì¼ ë•Œì˜ ì—°ë ¹
    """
```

### 5.3 ìì›í‰ê°€ ê¸°ì´ˆ ë¶„ì„

```python
def stock_assessment_basics(biomass: np.ndarray, fishing_mortality: np.ndarray) -> Dict[str, Any]:
    """
    ê¸°ì´ˆ ìì›í‰ê°€ ë¶„ì„
    
    ì§€í‘œ ê³„ì‚°:
    - ìƒì²´ëŸ‰ ì¶”ì„¸
    - ì–´íšì‚¬ë§ë¥  ë¶„ì„
    - ì§€ì†ê°€ëŠ¥ì„± ì§€í‘œ
    - ê´€ë¦¬ê¸°ì¤€ì  ê³„ì‚°
    """
```

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: í•„ìˆ˜ ê¸°ëŠ¥ (Week 1-8)
1. ê¸°ìˆ í†µê³„ (ì™„ì „ êµ¬í˜„)
2. ê°€ì • ê²€ì • (ì •ê·œì„±, ë“±ë¶„ì‚°ì„±)
3. t-ê²€ì • (3ì¢…ë¥˜)
4. ì¼ì›ë¶„ì‚°ë¶„ì„ + Tukey HSD
5. ë¹„ëª¨ìˆ˜ ê²€ì • (Mann-Whitney, Kruskal-Wallis)

### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (Week 9-11)
1. ì´ì›ë¶„ì‚°ë¶„ì„
2. ë‹¤ì¤‘ ì‚¬í›„ë¶„ì„ ë°©ë²•
3. ë‹¨ìˆœ/ë‹¤ì¤‘ íšŒê·€ë¶„ì„
4. ìƒê´€ë¶„ì„

### Phase 3: ì „ë¬¸ê°€ ê¸°ëŠ¥ (Week 12-13)
1. ê²€ì •ë ¥ ë¶„ì„
2. ìˆ˜ì‚°ê³¼í•™ íŠ¹í™” ë¶„ì„
3. ê³ ê¸‰ ì§„ë‹¨ ë„êµ¬

ëª¨ë“  ë¶„ì„ì€ **ì™„ì „í•œ ê°€ì • ê²€ì •, íš¨ê³¼í¬ê¸° ê³„ì‚°, ì‹ ë¢°êµ¬ê°„, í•´ì„ ê°€ì´ë“œ**ë¥¼ í¬í•¨í•˜ì—¬ **SPSS/R ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±**ì„ ì œê³µí•©ë‹ˆë‹¤.